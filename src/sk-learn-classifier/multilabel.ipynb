{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer,\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.dataset_db import dynamo_db\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\"\"\"\n",
    "Not used in optimal sol'n - also makes it take hours to run\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "\n",
    "def get_words(text):\n",
    "    text = text.lower()\n",
    "    wordlist = text.split()\n",
    "    clean_list = []\n",
    "    for word in wordlist:\n",
    "        # only get words (no digits)\n",
    "        if not word.isdigit() and not re.match(r\"[^\\w]\", word):\n",
    "            clean_list.append(word)\n",
    "\n",
    "    return \" \".join(clean_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Doesn't actually train, just generates a JSON in the format we want of training data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(dataset, matching_dataset):\n",
    "    next_index = 0\n",
    "    themes = {}  # themes to indices\n",
    "    targets = []  # indices of themes, parallel to text array\n",
    "    text = []\n",
    "    urls = []\n",
    "\n",
    "    i = 0\n",
    "    for project in dataset:\n",
    "        m_themes = project[\"themes\"]\n",
    "        for matching_project in matching_dataset:\n",
    "            if matching_project[\"url\"] == project[\"url\"]:\n",
    "                m_themes = matching_project[\"themes\"]\n",
    "                break\n",
    "        if len(project[\"text\"]) != 0:\n",
    "            words = get_words(project[\"text\"])\n",
    "            urls.append(project[\"url\"])\n",
    "            text.append(words)\n",
    "            targets.append([])\n",
    "            for theme in m_themes:\n",
    "                if theme[\"id\"] not in themes:\n",
    "                    themes[theme[\"id\"]] = next_index\n",
    "                    next_index += 1\n",
    "                targets[i].append(themes[theme[\"id\"]])\n",
    "        i += 1\n",
    "\n",
    "    data = {}\n",
    "    data[\"themes\"] = themes\n",
    "    data[\"targets\"] = targets\n",
    "    data[\"urls\"] = urls\n",
    "    data[\"text\"] = text\n",
    "    with open(\"trained1.json\", \"w\") as output_file:  # trained.json\n",
    "        json.dump(data, output_file)\n",
    "\n",
    "    return themes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Vectorizes both training and testing data, then classifies\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(testing_data, testing_targets):\n",
    "    with open(\"trained1.json\", \"r\") as input_file:  # trained.json\n",
    "        training_data = json.load(input_file)\n",
    "\n",
    "    urls = []\n",
    "    text = []\n",
    "    targets = []\n",
    "    i = 0\n",
    "    for project in testing_data:\n",
    "        if len(project[\"text\"]) != 0:\n",
    "            text.append(get_words(project[\"text\"]))\n",
    "            urls.append(project[\"url\"])\n",
    "            targets.append([])\n",
    "            for theme in project[\"themes\"]:\n",
    "                if theme[\"id\"] not in training_data[\"themes\"]:\n",
    "                    continue\n",
    "                targets[i].append(training_data[\"themes\"][theme[\"id\"]])\n",
    "        i += 1\n",
    "\n",
    "    text_clf = Pipeline(\n",
    "        [\n",
    "            (\"vect\", CountVectorizer(ngram_range=(1, 2), max_df=0.6)),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            # (\n",
    "            #     \"clf\",\n",
    "            #     SGDClassifier(random_state=42, max_iter=50, class_weight=\"balanced\"),\n",
    "            # ),\n",
    "             (\"clf\", OneVsRestClassifier(SGDClassifier(random_state=42, loss=\"log\")))\n",
    "#            (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y = training_data[\"targets\"]\n",
    "    y = MultiLabelBinarizer().fit_transform(y)\n",
    "    \n",
    "    test_words = text\n",
    "    text_clf.fit(training_data[\"text\"], y)\n",
    "    predicted = text_clf.predict_proba(test_words)\n",
    "    log_predicted = text_clf.predict(test_words)\n",
    "\n",
    "#     with open(\"predictions1.json\", \"w\") as output_file:  # predictions.json\n",
    "#         json.dump(\n",
    "#             predicted.tolist(),\n",
    "#             output_file,\n",
    "#             separators=(\",\", \":\"),\n",
    "#             sort_keys=True,\n",
    "#             indent=4,\n",
    "#         )\n",
    "\n",
    "    return predicted, log_predicted\n",
    "\n",
    "\n",
    "def get_targets(data, data_with_targets, themes):\n",
    "    targets = []\n",
    "    i = 0\n",
    "    for project in data:\n",
    "        m_themes = project[\"themes\"]\n",
    "        for matching_project in data_with_targets:\n",
    "            if matching_project[\"url\"] == project[\"url\"]:\n",
    "                m_themes = matching_project[\"themes\"]\n",
    "                break\n",
    "        targets.append([])\n",
    "        for theme in m_themes:\n",
    "            if theme[\"id\"] not in themes:\n",
    "                targets[i].append(-1)\n",
    "            targets[i].append(themes[theme[\"id\"]])\n",
    "        i += 1\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dynamo_db.get_dataset(\"organizations_text\")\n",
    "print(len(dataset))\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_dataset = dynamo_db.get_dataset(\"organizations\")\n",
    "print(len(matching_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecdev': 0, 'edu': 1, 'finance': 2, 'gender': 3, 'rights': 4, 'climate': 5, 'children': 6, 'env': 7, 'health': 8, 'human': 9, 'tech': 10, 'animals': 11, 'disaster': 12, 'sport': 13, 'art': 14, 'democ': 15, 'hunger': 16, 'lgbtq': 17}\n"
     ]
    }
   ],
   "source": [
    "themes = train(train_data, matching_dataset)\n",
    "print(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 1, 9], [6, 0, 1, 3, 8, 9, 4, 10, 16], [3], [6, 0, 1, 8, 13], [0, 1, 2, 3], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 10], [6, 1, 3, 9, 4, 10], [6, 5, 0, 1, 7, 3, 8, 4], [6, 1], [6, 5, 12, 0, 1, 7, 3, 8, 9, 4, 16, 14], [3], [6, 1, 3, 9, 4], [8], [11], [5, 0, 1, 7, 3, 10], [6, 5, 0, 1, 7, 3, 8, 9, 4], [6, 1], [6, 1, 3, 13], [6, 0, 1, 3, 8, 4], [1], [6, 1, 3, 9, 4, 16], [6, 1, 2, 3, 4], [6, 12, 1, 3, 9, 16], [6, 15, 0, 1, 7, 3, 8, 4, 10], [6, 0, 1, 7, 3, 8, 9, 16], [6, 5, 1, 7, 10], [6, 5, 12, 0, 1, 7, 3, 8, 9, 10], [0, 1, 8, 9], [6, 1, 8], [6, 0, 1, 2, 3, 8, 9, 4, 13, 14], [6, 0, 7, 3, 8, 9, 10], [6, 1, 3, 8, 9, 13], [1], [6, 1, 2, 3, 8, 4], [0, 1, 3], [0, 1, 7, 2, 3, 8, 9, 4], [6, 0, 1, 2, 3], [11, 6, 5, 15, 0, 1, 7, 2, 3, 8, 4, 10, 14], [6, 12, 0, 1, 7, 3, 8, 9], [1, 2], [6, 1], [6, 1, 3, 8, 4, 13], [6, 0, 1, 3, 8, 16], [6, 1, 4, 13], [5, 0, 1, 7, 3, 8, 4, 10], [6, 0, 1, 2, 3, 8, 13], [6, 0, 1, 2, 3, 8, 4, 16], [0, 7, 4], [12, 0, 1, 3, 14], [12, 1], [11, 0, 1, 7], [6, 1, 3, 10], [6, 1, 3, 16], [7, 8], [6, 0, 1, 3, 8], [6, 1, 3], [6, 1, 3, 4], [1, 8], [14], [6, 1, 3], [6, 15, 12, 0, 1, 3, 8, 9, 4, 16], [6, 1, 3, 8, 9], [4], [8, 9, 4], [6, 1, 3, 4], [12, 0, 7, 3, 8, 16], [6, 0, 1, 3, 8, 9, 4, 16], [11, 8], [6], [6], [6, 15, 0, 1, 3, 8, 9, 4, 13], [11, 5, 0, 7], [6, 16], [6, 1, 4], [6, 1, 3, 8, 9, 16], [6, 1, 8], [1, 7], [6, 1, 9, 4], [4], [6, 1, 3], [6, 1, 3], [6, 8, 9], [8], [6, 1, 2, 3, 8], [1], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 13, 10], [6, 15, 0, 1, 3, 8, 10], [15, 1, 3], [6, 0, 1, 3, 8, 4], [6, 0, 1, 2, 3, 8], [6, 1, 3], [8], [6, 1, 13, 10, 14], [11, 7], [11, 6, 5, 12, 0, 1, 7, 3, 8], [6, 1, 8], [6, 1, 8, 9], [6], [6, 1, 10], [6, 1, 3, 8, 9], [10], [8], [1], [6, 12, 1, 8, 16], [6, 15, 1, 2, 3, 8, 9, 4, 10], [8, 9, 4], [7], [6, 5, 15, 0, 1, 7, 2, 3, 4, 10, 14], [6, 1, 8, 9, 16], [0, 7, 3, 8, 16], [8], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 16], [6, 1, 3, 10], [6, 5, 12, 1, 2, 3, 8, 9], [1], [6, 0, 1, 3], [5, 0, 3, 8, 4], [1], [1, 3, 4], [6, 0, 8], [3, 4], [8], [15, 0, 1, 4], [7, 8], [6, 0, 1, 7, 2, 3, 9], [6, 0, 1, 3, 9, 16], [1, 3, 8], [6, 12, 1, 2, 3, 8, 4, 16], [1], [6, 0, 1, 3, 8, 4], [6], [15, 4], [6, 1, 3, 8], [6, 1, 3, 9, 4, 13, 16, 14], [6, 1, 8, 4], [1], [1, 3, 8, 9, 4], [11, 6, 5, 15, 0, 1, 7, 2, 3, 8, 4], [7], [11], [11, 7], [6, 15, 1, 3, 8, 4], [6, 12, 1, 8, 13, 10], [6, 0, 1, 2, 3, 8, 4, 16], [6, 15, 0, 2, 3, 8, 9, 4, 10, 14], [0, 1, 3, 4], [15, 1, 7, 8, 13], [5, 12, 0, 1, 7, 8, 4], [6, 3, 4], [6, 0, 1, 7, 3, 8, 4, 16], [6, 15, 0, 1, 3, 4, 13, 10], [6, 0, 1, 3], [6, 1, 8, 4, 16], [6, 1, 8, 4], [6, 12, 0, 1, 7, 3, 8], [6, 5, 0, 1, 7, 3, 8], [1, 8], [6, 0, 7, 3, 8, 9], [0, 1, 3, 4], [1], [6, 1, 8, 9], [0, 1, 2, 3, 8], [11], [1, 7], [6], [6, 0, 1, 7], [4], [1], [6, 0, 1, 2, 3], [5, 7, 4], [6, 3, 9, 4], [1], [6, 3, 9], [1, 8], [5, 1, 7, 8], [6, 0, 3, 8], [6, 1, 3], [0, 3, 8], [6, 0, 1, 3, 9], [6, 1, 3, 4], [6, 1, 3, 8], [5, 12, 0, 7, 2, 8, 9, 4, 10, 16], [6, 1, 3, 8], [3], [6, 8], [6, 5, 15, 12, 0, 1, 3, 8, 4, 10, 17], [8], [6, 0, 1, 3, 8, 9, 4], [1, 3], [6, 3, 8, 4, 16], [6, 0, 1, 2, 3, 8], [6, 5, 15, 12, 0, 1, 7, 3, 8, 9, 4, 16], [6, 0, 1, 3, 8, 13, 10], [6, 5, 0, 1, 7, 2, 3, 8, 4, 10, 16], [15, 0, 1, 2, 3], [6, 1, 9], [1, 7], [6, 3, 8, 10], [1], [6, 15, 0, 1, 3, 4, 10], [6, 8, 14], [6, 8], [6, 1, 3, 8, 4], [6, 15, 1, 3, 4, 10], [6, 0, 1, 3, 8, 16], [6, 0, 1, 2, 3, 9, 16], [0, 4], [6, 1, 13], [6, 3, 4], [6, 1, 7, 3, 8, 13, 10], [6, 5, 0, 1, 7, 2, 8, 10], [6, 1, 3, 8, 9, 4, 14], [6, 0, 1, 3, 8, 4, 10], [6, 15, 12, 0, 1, 3, 8, 4], [15, 1, 4], [6, 1, 3], [6, 3, 13], [6, 1, 7], [6, 0, 1, 8, 9], [5, 1, 3, 8, 4, 10], [6, 15, 0, 1, 7, 8, 10], [3], [8], [6, 0, 1, 7, 2, 3, 8, 9, 4, 13, 10], [6, 15, 0, 1, 7, 2, 3, 8, 9, 4, 10], [7], [6, 5, 0, 1, 7, 3, 8, 10], [6, 1, 3], [1], [11, 1, 7], [6, 3, 8], [6, 5, 12, 1, 7, 8, 4, 14], [6, 8], [1], [6, 1, 8, 14], [0, 1], [15, 0, 1, 3, 4], [6, 0, 1, 3, 4], [6, 5, 12, 0, 1, 7, 3, 8, 4, 16], [3], [6, 0, 1, 7, 3, 8, 10], [1, 3, 8, 13], [6, 1, 14], [3, 8], [1], [3, 14], [6, 1, 3, 4, 14], [6, 5, 12, 0, 1, 2, 3, 8, 9, 4], [11, 6, 5, 12, 0, 1, 7, 3, 8, 9, 10], [6, 5, 15, 12, 0, 1, 7, 3, 8, 9, 4, 13, 16, 14, 17], [6, 0, 1, 7, 2, 3, 8, 4, 16], [6, 0, 1, 3, 8, 9], [6, 12, 1, 7, 2, 3, 8, 4, 14], [1, 8], [6, 5, 15, 1, 7, 2, 3, 8, 10], [0], [6, 0, 1, 4], [1, 3, 4], [6, 1, 3, 9, 4], [15, 12, 0, 1, 3, 8, 4, 10], [6, 0, 1, 3, 8, 9], [3, 4, 10], [11, 7], [6, 15, 1, 7, 3, 4], [6, 5, 1, 7, 3, 8], [8], [6, 12, 0, 1, 7, 3, 8, 9, 4, 13], [0, 1, 2, 3], [11, 6, 15, 0, 1, 7, 3, 8, 9, 4, 10, 16, 14], [6, 5, 0, 1, 7, 3, 8], [6, 12, 3, 8, 9], [0, 1, 3], [6, 3, 8], [0, 1, 7, 3, 8, 4], [6, 0, 1, 7, 2, 3, 8, 13, 10, 16, 14], [1, 2, 8], [15, 7, 3, 4], [6, 5, 12, 0, 1, 7, 3, 8, 4, 10, 16], [4], [0], [6, 8, 16], [5, 0, 7, 8], [6], [5, 15, 0, 1, 7, 2, 3, 8, 9, 4], [6, 12, 0, 1, 7, 8, 9, 13], [11, 1, 8], [6, 5, 1, 7, 3, 8, 4, 10, 14], [6, 1], [7], [0, 1], [8], [1], [6, 5, 0, 1, 7, 3, 8, 9, 4], [6, 1, 3, 8, 4], [6, 0, 1, 7, 3, 8, 4, 13], [0], [15, 0, 1, 2], [0, 1, 7], [1], [6, 0, 1, 7, 2, 3, 8, 4], [6, 0, 1, 3, 8, 9, 4], [3, 8], [0, 1, 3], [8], [1], [6, 2, 8, 4], [6, 0, 1, 7, 3, 8, 13, 10], [6, 12, 0, 1, 3], [6, 0, 1, 3, 8, 9, 4, 13, 16, 14], [7], [6, 1, 3, 8, 9], [0, 2], [6, 5, 0, 1, 7, 8, 10, 16, 14], [6, 0, 1, 2, 3, 8, 9, 16], [1], [6, 1, 3, 8], [6, 15, 12, 0, 1, 7, 3, 8, 9, 4, 13], [11], [0], [6, 12, 1, 2, 8, 4], [6, 12, 8, 9, 16], [1, 7, 8, 13], [6, 0, 1, 3, 8], [6], [12, 0, 8, 9, 4], [6, 12, 0, 1, 7, 3, 8, 4], [6, 1, 3, 8, 9], [6], [6, 1, 3, 8, 4], [6, 3, 8, 9, 4], [0, 1, 2, 3, 8, 10, 16], [0, 1, 7, 2, 3, 9], [1, 3], [11, 5, 0, 7, 8, 14], [8], [6, 15, 0, 1, 2, 3, 8, 9, 4, 13], [0, 4], [6, 8], [6, 0, 1, 3, 8, 14], [6, 0, 1, 2, 3, 8, 9, 4, 10], [12, 3, 9], [6, 5, 15, 1, 7, 2, 3, 8, 9], [3, 8], [1, 8], [6, 1, 3, 4], [6, 0, 1, 3, 13], [5, 0, 1, 7], [11, 5, 7, 14], [11, 1, 7], [6, 15, 0, 1, 7, 2, 3, 8, 4], [11, 6, 5, 0, 1, 7, 8, 14], [0, 3], [6, 5, 15, 12, 0, 1, 2, 3, 8, 9, 4, 16, 14], [1, 10, 14], [1], [3], [6, 1, 3, 8, 16], [1, 7, 8, 16], [6, 1, 8], [6, 0, 1, 2, 3, 8, 4], [6, 1, 3, 8], [0, 2, 3], [11, 6, 5, 0, 1, 7, 3, 8, 9], [6, 1, 3], [6, 0, 1, 7, 2, 3, 8, 9, 10], [6, 0, 1], [11, 1, 7], [6, 3, 8, 9], [8], [15], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 14], [6, 15, 12, 0, 1, 2, 3, 8, 9, 4, 10, 16], [6, 1, 3, 4, 14], [0], [11, 6, 1, 8], [15, 0, 1, 3, 8, 4], [6, 15, 7, 3, 8, 4], [7], [8], [3, 8], [6, 5, 0, 1, 7, 3, 8, 4], [4], [0, 1, 3], [15, 0, 1, 8, 4], [6, 0, 8, 10], [1, 3], [6, 5, 15, 0, 1, 7, 2, 3, 8, 10, 16], [8], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4], [12, 0, 1, 8], [6, 0, 1, 3, 8], [6, 3, 4], [8], [1, 8], [6, 4], [6, 1, 3, 8], [15, 12, 0, 1, 7, 3, 9, 4], [15, 1, 4, 10, 14], [0, 3, 8, 4, 17], [0, 1, 3], [6, 8], [6, 5, 1, 7, 3, 10], [6], [6, 3], [0, 1, 2, 3], [6, 0, 1, 3], [6, 1, 3, 8, 16], [8], [5, 7], [5, 0, 1, 7, 3, 8, 4, 10], [6, 0, 1, 8], [11, 6, 12, 0, 1, 7, 2, 3, 8, 16], [6, 1, 3, 8, 4], [8], [0, 1, 3, 4], [6, 15, 0, 1, 2, 3, 8, 4], [8], [6, 1, 3, 13, 14], [6, 0, 1, 3, 8, 9, 4, 10, 16], [6, 0, 1, 2, 3, 8, 4, 13, 16], [6, 1, 2, 4], [12, 0, 1], [6, 8], [5, 0, 1, 7, 8, 4, 10], [6, 1, 3, 10], [6, 1, 9], [3], [6, 12, 1, 3, 8], [1, 3], [6, 1, 10], [3, 8, 4], [12, 8, 9], [6, 15, 0, 1, 2, 3, 8, 9, 4], [1, 7, 8], [6, 1, 3, 9, 14], [1, 2, 8], [15, 0, 1, 10], [6, 1, 8, 13], [1], [6, 0, 1, 7, 2, 3, 8, 13], [6, 12, 0, 1, 14], [6, 1], [6, 0, 1, 3], [7], [1, 2, 3, 10], [11, 6, 5, 0, 1, 7, 2, 3, 8, 4], [7], [1], [11, 6, 0, 1, 7, 3, 14], [6, 15, 0, 1, 3, 8, 4], [6, 1, 8, 9], [6, 0, 1, 2, 3, 8, 4], [1], [11], [6, 5, 0, 1, 7, 2, 3, 8, 9], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 10, 16, 14], [1, 2, 3, 8, 4, 13], [11, 6, 12, 0, 1, 2, 3, 8, 9, 4, 10], [6, 1, 3, 8, 4], [6, 1, 3, 8, 4], [5, 0, 1, 7], [6, 12, 0, 1, 7, 3, 8, 13, 10], [6, 12, 1, 7, 2, 10], [6, 12, 1, 2, 3, 8, 9, 4, 16], [6, 1, 7, 3, 8, 13], [6, 15, 0, 1, 2, 3, 8, 9, 4, 13, 10, 16, 14], [6, 1, 3, 8], [6, 5, 0, 7, 3, 16], [6, 8], [5, 7, 16], [6, 0, 1, 2, 3, 8, 9, 4], [6, 4], [6, 1, 7], [1, 8], [6, 1], [6, 1, 3, 8, 13], [6, 12, 0, 1, 2, 3, 8, 9, 4, 14], [6, 1, 3, 8, 4], [6, 1, 8, 13], [6, 1], [8], [6, 8], [0, 1, 8], [1, 3, 8], [6, 1, 3, 9, 4], [11, 6, 5, 0, 1, 7, 3, 8, 9, 4, 13, 16], [6, 0, 1, 3, 9, 4, 13], [6, 15, 1, 3, 4], [6, 15, 4], [6, 15, 0, 1, 8, 4], [15, 0, 4], [6, 7, 8, 9, 4], [1], [6, 5, 15, 0, 1, 7, 3, 4], [6, 1, 3, 8, 10], [1, 3, 4], [6, 1, 3, 16], [1], [0], [0, 1, 3, 10], [6, 12, 1, 8, 9, 13, 10], [15, 12, 3], [11, 6, 3, 8, 13], [0, 1, 2, 4], [6, 0, 1, 7, 3, 8], [6, 5, 0, 1, 7, 3, 8, 16], [6, 5, 15, 12, 1, 7, 3, 8, 9, 4, 16, 17], [13], [6, 1], [6, 1, 3, 8, 9, 4, 13, 16], [6, 12, 3, 8, 9], [8, 9, 4], [1], [6, 5, 1, 3, 13], [6, 1, 3, 8, 9], [5, 7], [1, 10], [4], [0, 1, 3], [6, 8], [6, 8], [1], [6], [6, 1, 3, 8, 16], [11, 6, 5, 12, 1, 7, 3, 8, 9, 10, 14], [6, 1, 7, 8, 13], [8], [0, 2, 8, 16], [5, 12, 0, 7, 2, 9, 10], [6, 12], [1, 3, 8], [6, 3, 8, 10], [1], [6, 0, 1, 3, 16], [6, 1, 3, 8], [6, 1, 8], [11, 6, 0, 1, 2, 3, 8, 16], [6, 0, 1, 2, 3, 8, 4], [1], [1], [6, 0, 1], [6, 0, 1, 3, 14], [1, 3, 4], [6, 12, 0, 1, 7, 3, 8, 9, 4, 16], [1], [7, 4, 10], [11, 6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 14], [0, 1], [6, 1], [11, 7, 14], [6, 1, 7, 3, 10, 14], [11, 6, 15, 12, 0, 1, 7, 3, 8, 4, 16], [6, 12, 0, 1, 3, 8, 9, 10, 16], [0, 1, 3, 8, 9, 4], [12], [1], [5, 1, 7, 2, 3, 8, 13, 10], [6, 15, 1, 3, 4], [3], [6, 0, 1, 3, 4], [8, 4], [0, 1, 7, 2, 3, 8], [5, 15, 7, 3], [8], [6, 0, 1, 8, 13], [15, 1, 10], [6, 1], [6, 0, 1, 3, 8, 9], [10], [6, 12, 0, 1, 3, 8, 9, 4], [6, 12, 0, 1, 3, 8, 9], [1, 4, 17], [6, 5, 1, 3, 8, 4, 16, 14], [11, 6, 5, 15, 12, 0, 1, 7, 3, 8, 9, 4], [0, 3, 4, 10], [6, 1, 8], [6, 0, 1, 7, 8, 13, 10, 14], [3, 4], [15, 0, 3, 8, 9, 4], [6, 1, 8, 4], [6, 1, 3, 4, 16], [6, 7], [1, 3, 8], [1], [6, 12, 0, 1, 2, 3, 8, 9, 4], [15, 0, 7, 2, 3, 8, 4], [6, 15, 0, 1, 7, 2, 3, 8, 9, 4, 10], [6, 1, 7], [0, 2], [11, 5, 7, 16], [6, 1, 8], [1], [6, 0, 1, 2, 8, 16], [11, 1, 7], [6], [5, 0, 7, 8], [1, 3], [6, 1, 7, 3, 8, 9, 4], [0, 1, 3, 10], [11, 5, 0, 1, 7, 3, 9], [6, 3], [6, 0, 1], [6, 5, 0, 1, 7, 2, 3, 8, 9, 4], [6, 1, 7, 3, 8, 9], [6], [6, 8], [6, 1, 7, 8], [6, 8, 13], [11, 6, 0, 1], [6, 1, 8], [6, 8], [1, 7, 8, 4], [6, 1, 16], [6, 5, 15, 1, 3, 8, 9, 4, 14], [0], [1, 8], [11, 6, 1, 7, 14], [6, 0, 1, 3, 8, 4], [6, 0, 1, 7, 2, 3, 8, 9, 13, 10], [8], [1, 7, 3], [4], [1, 9], [12], [12, 7, 8, 9], [6, 1], [6, 5, 15, 1, 2, 3, 8, 4], [6, 12, 0, 1, 3, 8, 10], [6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 10], [0, 1, 7, 8], [6, 5, 15, 12, 0, 1, 7, 3, 8, 9, 4], [6, 0, 1, 7, 3, 8], [1, 7, 3, 10], [6, 1, 3], [6, 3, 8], [2, 3, 8], [6, 15, 12, 1, 7, 3, 8, 9, 4, 13], [6, 3, 8], [6, 15, 1, 9, 4, 14], [6, 5, 15, 0, 7, 3, 8, 16], [7], [6, 12, 0, 1, 7, 3, 8, 9, 4, 16], [12], [6, 15, 0, 1, 8, 4, 14], [6, 1, 8], [11], [1], [6, 0, 1, 3, 8], [11, 6, 1, 3, 8], [6], [5], [6, 0, 1, 3, 8, 9, 4], [11, 6, 15, 1, 3, 8, 4, 13], [0, 1, 8, 10], [3], [6, 15, 0, 1, 3, 8, 4], [0, 1, 7, 2, 3, 8], [6, 3, 8], [6, 8], [3], [6, 1, 3, 8, 4, 10, 16], [0, 1, 7, 3], [7, 3, 8, 10], [11, 6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 16], [6, 15, 0, 1, 3, 4], [11, 6, 5, 15, 12, 0, 1, 7, 2, 3, 8, 9, 4, 13, 10, 16, 14], [8], [5, 1, 3, 4], [6, 0, 1, 3, 8, 4, 10, 14], [0, 1, 8, 9], [6, 1, 8, 16], [6, 1], [6, 0, 1, 7, 3, 8, 4], [1, 8, 9], [0, 7, 3, 8, 4], [6, 1, 3, 8, 4], [6, 0, 1, 3, 8, 13], [0], [1, 8], [6, 0, 1, 2, 3, 8], [6, 0, 1, 7, 2, 3, 8, 4, 10], [0], [6, 5, 0, 1, 7, 3, 9, 4, 16, 14], [6, 15, 0, 3, 8, 4]]\n"
     ]
    }
   ],
   "source": [
    "testing_targets = get_targets(test_data, matching_dataset, themes)\n",
    "print(testing_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24138187 0.93163593 0.05905105 ... 0.11105406 0.06367558 0.00766928]\n",
      " [0.65527132 0.76905202 0.20598563 ... 0.08486023 0.09889555 0.00608746]\n",
      " [0.29054517 0.65003444 0.08946829 ... 0.03780805 0.11125251 0.00717774]\n",
      " ...\n",
      " [0.32604578 0.25301279 0.06052227 ... 0.03427857 0.08634965 0.00870725]\n",
      " [0.79367794 0.79417985 0.24737021 ... 0.21563612 0.13775254 0.00531084]\n",
      " [0.56765046 0.75959952 0.19742883 ... 0.1176932  0.13259303 0.00546089]]\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "probabilities, predictions = classify(test_data, testing_targets)\n",
    "print(probabilities)\n",
    "print(predictions)\n",
    "print(len(testing_targets))\n",
    "# print(set(testing_targets))\n",
    "# print(np.mean(predictions == testing_targets))\n",
    "# print(metrics.confusion_matrix(testing_targets, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24138187 0.93163593 0.05905105 0.4260415  0.21547512 0.06678046\n",
      " 0.67277869 0.13913992 0.34276627 0.19255453 0.18798987 0.04413205\n",
      " 0.07105612 0.08532089 0.07787218 0.11105406 0.06367558 0.00766928]\n",
      "[0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 6, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(probabilities[0])\n",
    "print(predictions[0])\n",
    "testing_targets[0].sort()\n",
    "testing_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65527132 0.76905202 0.20598563 0.83068604 0.26963452 0.16457732\n",
      " 0.55105735 0.45727434 0.59985664 0.2202207  0.119725   0.04965593\n",
      " 0.14895674 0.04327176 0.04688075 0.08486023 0.09889555 0.00608746]\n",
      "[1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 6, 8, 9, 10, 16]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(probabilities[1])\n",
    "print(predictions[1])\n",
    "testing_targets[1].sort()\n",
    "testing_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecdev:\n",
      "Accuracy: 0.7002923976608187\n",
      "Precision: 0.6813186813186813\n",
      "Recall: 0.4575645756457565\n",
      "F1: 0.5474613686534217\n",
      "    T  F\n",
      "T  124, 58\n",
      "F  147, 355\n",
      "\n",
      "edu:\n",
      "Accuracy: 0.7353801169590644\n",
      "Precision: 0.7342767295597484\n",
      "Recall: 0.9749478079331941\n",
      "F1: 0.8376681614349777\n",
      "    T  F\n",
      "T  467, 169\n",
      "F  12, 36\n",
      "\n",
      "finance:\n",
      "Accuracy: 0.847953216374269\n",
      "Precision: 0.5\n",
      "Recall: 0.009615384615384616\n",
      "F1: 0.01886792452830189\n",
      "    T  F\n",
      "T  1, 1\n",
      "F  103, 579\n",
      "\n",
      "gender:\n",
      "Accuracy: 0.6681286549707602\n",
      "Precision: 0.6607538802660754\n",
      "Recall: 0.8010752688172043\n",
      "F1: 0.7241798298906439\n",
      "    T  F\n",
      "T  298, 153\n",
      "F  74, 159\n",
      "\n",
      "rights:\n",
      "Accuracy: 0.7207602339181286\n",
      "Precision: 0.6923076923076923\n",
      "Recall: 0.27876106194690264\n",
      "F1: 0.39747634069400634\n",
      "    T  F\n",
      "T  63, 28\n",
      "F  163, 430\n",
      "\n",
      "climate:\n",
      "Accuracy: 0.8801169590643275\n",
      "Precision: 0.9\n",
      "Recall: 0.1\n",
      "F1: 0.18000000000000002\n",
      "    T  F\n",
      "T  9, 1\n",
      "F  81, 593\n",
      "\n",
      "children:\n",
      "Accuracy: 0.6476608187134503\n",
      "Precision: 0.6481481481481481\n",
      "Recall: 0.8728179551122195\n",
      "F1: 0.7438894792773645\n",
      "    T  F\n",
      "T  350, 190\n",
      "F  51, 93\n",
      "\n",
      "env:\n",
      "Accuracy: 0.8070175438596491\n",
      "Precision: 0.8311688311688312\n",
      "Recall: 0.34972677595628415\n",
      "F1: 0.49230769230769234\n",
      "    T  F\n",
      "T  64, 13\n",
      "F  119, 488\n",
      "\n",
      "health:\n",
      "Accuracy: 0.6871345029239766\n",
      "Precision: 0.6953316953316954\n",
      "Recall: 0.7587131367292225\n",
      "F1: 0.7256410256410256\n",
      "    T  F\n",
      "T  283, 124\n",
      "F  90, 187\n",
      "\n",
      "human:\n",
      "Accuracy: 0.7880116959064327\n",
      "Precision: 0.4\n",
      "Recall: 0.05673758865248227\n",
      "F1: 0.09937888198757765\n",
      "    T  F\n",
      "T  8, 12\n",
      "F  133, 531\n",
      "\n",
      "tech:\n",
      "Accuracy: 0.8596491228070176\n",
      "Precision: 0.5\n",
      "Recall: 0.020833333333333332\n",
      "F1: 0.039999999999999994\n",
      "    T  F\n",
      "T  2, 2\n",
      "F  94, 586\n",
      "\n",
      "animals:\n",
      "Accuracy: 0.9473684210526315\n",
      "Precision: 1.0\n",
      "Recall: 0.23404255319148937\n",
      "F1: 0.3793103448275862\n",
      "    T  F\n",
      "T  11, 0\n",
      "F  36, 637\n",
      "\n",
      "disaster:\n",
      "Accuracy: 0.8771929824561403\n",
      "Precision: 0.4\n",
      "Recall: 0.024096385542168676\n",
      "F1: 0.045454545454545456\n",
      "    T  F\n",
      "T  2, 3\n",
      "F  81, 598\n",
      "\n",
      "sport:\n",
      "Accuracy: 0.9166666666666666\n",
      "Precision: 1.0\n",
      "Recall: 0.017241379310344827\n",
      "F1: 0.03389830508474576\n",
      "    T  F\n",
      "T  1, 0\n",
      "F  57, 626\n",
      "\n",
      "art:\n",
      "Accuracy: 0.922514619883041\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "    T  F\n",
      "T  0, 0\n",
      "F  53, 631\n",
      "\n",
      "democ:\n",
      "Accuracy: 0.8728070175438597\n",
      "Precision: 0.5\n",
      "Recall: 0.022988505747126436\n",
      "F1: 0.04395604395604396\n",
      "    T  F\n",
      "T  2, 2\n",
      "F  85, 595\n",
      "\n",
      "hunger:\n",
      "Accuracy: 0.8830409356725146\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.012658227848101266\n",
      "F1: 0.024390243902439022\n",
      "    T  F\n",
      "T  1, 2\n",
      "F  78, 603\n",
      "\n",
      "lgbtq:\n",
      "Accuracy: 0.9926900584795322\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0\n",
      "    T  F\n",
      "T  0, 0\n",
      "F  5, 679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme_name,theme_number in themes.items():\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i][theme_number] == 1 and theme_number in testing_targets[i]:\n",
    "            tp += 1\n",
    "        if predictions[i][theme_number] == 1 and theme_number not in testing_targets[i]:\n",
    "            fp += 1\n",
    "        if predictions[i][theme_number] == 0 and theme_number in testing_targets[i]:\n",
    "            fn += 1\n",
    "        if predictions[i][theme_number] == 0 and theme_number not in testing_targets[i]:\n",
    "            tn += 1\n",
    "    \n",
    "    accuracy = (tp + tn) / len(predictions)\n",
    "    precision = tp / (tp+fp) if (tp+fp) != 0 else 0\n",
    "    recall = tp / (tp+fn) if (tp+fn) != 0 else 0\n",
    "    f1 = 2*((precision*recall) / (precision+recall)) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    print(theme_name + \":\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\",precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1:\",f1)\n",
    "    print(\"    T  F\")\n",
    "    print(\"T  \"+str(tp)+\", \"+str(fp))\n",
    "    print(\"F  \"+str(fn)+\", \"+str(tn))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5767068503605232\n",
      "[0.8, 0.7142857142857143, 0.4, 0.6666666666666665, 0.6666666666666665, 0.631578947368421, 0.6, 0.7692307692307693, 0.8, 0.7368421052631579, 0.5, 0.6666666666666665, 0.6666666666666666, 0, 0.6666666666666666, 0.6153846153846153, 0.5714285714285715, 0.6666666666666666, 0.8, 0.33333333333333337, 0.5454545454545454, 0.6666666666666665, 0.4444444444444444, 0.5, 0.6666666666666666, 0.5714285714285715, 0.7499999999999999, 0.6666666666666665, 0.8, 0.33333333333333337, 0.4, 0.8, 0.4, 0.8, 0.8571428571428571, 0.36363636363636365, 0.8000000000000002, 0.7000000000000001, 0.7692307692307693, 0.28571428571428575, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6153846153846154, 0.7692307692307692, 0.7692307692307693, 0.8, 0.5714285714285715, 0.4, 0.5, 0.4, 0.8571428571428571, 0.4, 0.888888888888889, 0.5, 1.0, 1.0, 0, 0.8571428571428571, 0.7499999999999999, 0.888888888888889, 0.2857142857142857, 0.28571428571428575, 0.75, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.4, 0.33333333333333337, 0.7142857142857143, 0, 0.25, 0.5, 0.5, 0.8, 0.4, 0.5714285714285715, 0, 0.7499999999999999, 0.8, 0.3333333333333333, 0.4, 0.888888888888889, 0.5, 0.6666666666666666, 0.8333333333333333, 0.3333333333333333, 0.8, 0.9090909090909091, 1.0, 0.4, 0.5, 0.6666666666666666, 0.19999999999999998, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8, 0.7499999999999999, 0, 0.5, 0.6666666666666666, 0.6666666666666665, 0.5714285714285714, 0.28571428571428575, 0, 0.16666666666666669, 0.5714285714285715, 0, 0, 0.7000000000000001, 0.8571428571428571, 0.5, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 0.33333333333333337, 0.5714285714285715, 0.4, 0.6666666666666666, 0, 0.3333333333333333, 0.5714285714285715, 0.25, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.2857142857142857, 0.5, 0.6666666666666666, 0.3333333333333333, 1.0, 0.36363636363636365, 0.6666666666666666, 1.0, 0.5, 0.7058823529411764, 0, 1.0, 0.6666666666666666, 0.8, 0.5, 0.4, 0.16666666666666669, 0.5714285714285715, 0.5, 0.36363636363636365, 0.8571428571428571, 0.2222222222222222, 0.4, 0.8571428571428571, 0.6666666666666665, 0.8571428571428571, 0.4444444444444445, 0.6, 0.6666666666666666, 0.6, 0.75, 1.0, 0.75, 0.5714285714285715, 0, 0.4, 0.6666666666666666, 0.5, 0, 0.5, 0.7499999999999999, 0.4, 0.5, 0.2857142857142857, 0.4, 0.8, 0.5714285714285715, 0.5714285714285715, 1.0, 0.6666666666666666, 0.6666666666666665, 0, 0.8571428571428571, 0.18181818181818182, 0.75, 0.4, 0.5, 0.7058823529411764, 0.6666666666666666, 0.7272727272727273, 0.6666666666666666, 0.6666666666666665, 0.2857142857142857, 0.5882352941176471, 0.25, 0.7058823529411764, 0.3636363636363636, 0.5714285714285715, 0.5714285714285715, 0.5714285714285715, 0.5, 0.6, 0.6666666666666666, 0, 0.888888888888889, 0.4444444444444444, 0.9090909090909091, 0.7272727272727273, 0, 0.8, 0.6666666666666666, 0.6666666666666666, 0.6153846153846154, 0.25, 0.7272727272727273, 0.7692307692307693, 0.3333333333333333, 0.8, 0.5, 0.8571428571428571, 0.6666666666666665, 0.8, 0.4444444444444445, 0.4, 0.4, 0.16666666666666669, 0.5333333333333333, 0.6666666666666666, 0.7692307692307693, 1.0, 0.5, 0.8, 0.8, 0.4, 0.6666666666666666, 0.4, 0.6666666666666666, 0.5, 1.0, 0.5, 0.7499999999999999, 0.5, 0.8333333333333333, 0.5714285714285715, 0.5, 0.4, 0.2857142857142857, 0.28571428571428575, 0.888888888888889, 0.7058823529411764, 0.7000000000000001, 0.5714285714285715, 0.7142857142857143, 0.8, 0.7142857142857143, 0.6666666666666666, 0.6666666666666667, 0, 0.5714285714285715, 0.8, 0.7272727272727272, 0.36363636363636365, 0.8, 0.6666666666666666, 0.8, 0.5, 0.7272727272727272, 0.4, 0.5714285714285715, 0.75, 0.631578947368421, 0.7272727272727273, 0.888888888888889, 0.8571428571428571, 0.5714285714285715, 0.6, 0.3076923076923077, 0.4, 0, 0.7777777777777778, 0, 0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.5714285714285715, 0.6666666666666666, 0.5714285714285715, 0.6153846153846153, 1.0, 0, 0.4444444444444445, 0.5, 0.6666666666666666, 0.5, 0.888888888888889, 0.8571428571428571, 0.5, 0.5714285714285715, 0.28571428571428575, 1.0, 0.6666666666666666, 0.923076923076923, 0.5, 0.4, 0.2857142857142857, 0.5, 0.5, 0.5454545454545454, 0.7499999999999999, 0.5714285714285715, 1.0, 0.888888888888889, 0.4, 0.3333333333333333, 0.6666666666666666, 0.5, 1.0, 0.7058823529411764, 1.0, 0.2857142857142857, 0.6, 0.7499999999999999, 0.4, 0.5714285714285715, 0.33333333333333337, 0, 0.5454545454545454, 0.7499999999999999, 0.4, 0.8000000000000002, 0.6666666666666665, 0.4444444444444445, 0.5454545454545454, 0.5714285714285715, 0.6, 0.5, 0.7499999999999999, 0.3333333333333333, 0.8, 0.6666666666666666, 0.5, 0.6, 0.6153846153846153, 0, 0.5, 0.888888888888889, 0.5, 0.3333333333333333, 0, 0.6666666666666666, 0.7142857142857143, 0.6153846153846154, 0.3333333333333333, 0.47058823529411764, 0.4, 0.6666666666666666, 0, 0.7499999999999999, 0.28571428571428575, 0.8571428571428571, 0.7272727272727273, 0.6666666666666666, 0.5, 0.6153846153846153, 0.7499999999999999, 0.8, 0.8, 0.28571428571428575, 0.5714285714285715, 0.5, 0, 0.5555555555555556, 0.5, 0.888888888888889, 1.0, 0.6666666666666666, 0.9090909090909091, 0.6666666666666666, 0.4, 0.4, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.4444444444444445, 0.28571428571428575, 0.5714285714285715, 0.6666666666666665, 0.5, 0.7368421052631579, 0.5, 1.0, 0.8571428571428571, 0.4, 0.6666666666666666, 0.4, 1.0, 0.4615384615384615, 0.33333333333333337, 0.6, 0.8, 0.5, 0.2857142857142857, 0.6666666666666666, 0.6666666666666666, 0.6666666666666665, 0.888888888888889, 0.888888888888889, 0.5, 0, 0.36363636363636365, 0.888888888888889, 0.5714285714285715, 0.7499999999999999, 0.6666666666666666, 0.5714285714285715, 0.5454545454545454, 0.5, 0.5714285714285715, 0.7142857142857143, 0.6153846153846153, 0.6666666666666665, 0.8, 0.8, 0.36363636363636365, 0.6666666666666666, 0.5, 0.2857142857142857, 0.7272727272727272, 0.5, 0.8, 0, 0.4, 0.8, 0.6666666666666666, 0.5714285714285715, 0.4, 0.22222222222222224, 0.8571428571428571, 0.4, 0.875, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5, 0.5714285714285715, 0, 0.5, 0.4444444444444445, 0.6, 0.75, 0.7272727272727273, 0.4, 0.6666666666666666, 0.7142857142857143, 0.6956521739130436, 0.4444444444444444, 0.625, 0.888888888888889, 0.7499999999999999, 0.7272727272727273, 0.6153846153846153, 0.30769230769230765, 0.5714285714285714, 0.7272727272727272, 0.47058823529411764, 0.888888888888889, 0.6666666666666666, 0.8, 0.2222222222222222, 0.7692307692307693, 0.5714285714285715, 0.5714285714285715, 0.6666666666666666, 0.8, 0.888888888888889, 0.5714285714285715, 0.888888888888889, 0.8571428571428571, 0.8, 0.33333333333333337, 0.8, 0.5714285714285715, 0.4, 0.5714285714285715, 0.7368421052631579, 0.4, 0.5714285714285715, 0.5714285714285715, 0.8333333333333334, 0.4444444444444444, 0.28571428571428575, 1.0, 0.6153846153846154, 0.5, 0.4, 0.6666666666666666, 1.0, 0, 0.6666666666666665, 0.36363636363636365, 0.3333333333333333, 0.5, 0.5, 0.8, 0.8571428571428571, 0.761904761904762, 0, 0.8, 0.6666666666666666, 0.5714285714285715, 0.4, 0.4, 0.6666666666666665, 0.888888888888889, 0.5, 0.6666666666666666, 1.0, 0.8, 1.0, 0.8, 0.33333333333333337, 0.6666666666666666, 0.7499999999999999, 0.6666666666666665, 0.6666666666666665, 0, 0.4, 0, 0.28571428571428575, 1.0, 0.5714285714285715, 0.5, 0.888888888888889, 0.8571428571428571, 0.8571428571428571, 0.6666666666666666, 0.923076923076923, 0.4, 0.6666666666666666, 0.6666666666666666, 0.6666666666666665, 0.6666666666666666, 0.5714285714285715, 0.4, 0, 0.6, 0.6666666666666666, 0.8, 0.3333333333333333, 0.4444444444444444, 0.7777777777777778, 0.7142857142857143, 0.6, 0, 0.33333333333333337, 0.5, 0.33333333333333337, 0.33333333333333337, 0.8000000000000002, 0, 0.4, 0.5714285714285715, 0.5, 0.7499999999999999, 0.4, 0.6666666666666666, 0.5, 0, 0.6666666666666666, 0.7692307692307692, 0.5, 0.7692307692307693, 0.7368421052631579, 0.5714285714285715, 0.7499999999999999, 0.4, 0, 0.5454545454545454, 0.75, 0.7499999999999999, 0.3333333333333333, 0.8571428571428571, 0.4, 0.7142857142857143, 0.6153846153846153, 0.625, 0.5, 0.4, 0.3333333333333333, 1.0, 0.5, 0.6, 1.0, 0.5, 0, 0.8, 0.8333333333333333, 0.6666666666666665, 0.4444444444444445, 0.6666666666666666, 0.6666666666666666, 0.7499999999999999, 0.8, 0.6666666666666666, 0.6666666666666666, 0.6666666666666665, 0.4, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 0.28571428571428575, 0.5714285714285715, 0.5714285714285714, 0.4, 1.0, 0.7499999999999999, 0.2857142857142857, 0.5714285714285715, 0.33333333333333337, 0.7499999999999999, 0, 0.5, 0, 0.28571428571428575, 0.6666666666666666, 0.6666666666666666, 0.6, 0.2666666666666667, 0.8, 0.625, 0.923076923076923, 0.5714285714285715, 0.8571428571428571, 0.8571428571428571, 0.3333333333333333, 0.4615384615384615, 0.8571428571428571, 0.4, 0, 0, 0.5714285714285715, 0, 0.4444444444444445, 1.0, 0, 0.5, 0.5714285714285715, 0.5714285714285715, 0.4, 0, 0.7272727272727273, 0.6666666666666666, 0.5, 0.4, 0.7272727272727273, 0.7272727272727272, 0.8571428571428571, 0.4444444444444445, 1.0, 0.7692307692307692, 1.0, 0.5, 0.4444444444444445, 0.2857142857142857, 0.6923076923076924, 1.0, 0.4444444444444445, 0.4, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.7272727272727273, 0.5714285714285715, 0.8333333333333333, 0.888888888888889, 0.9090909090909091, 0.2857142857142857, 0.8, 0.8, 0.5, 0, 0.625, 0.7272727272727272]\n",
      "0.8196881091617934\n",
      "[0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.5555555555555556, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 1.0, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 1.0, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.5555555555555556, 1.0, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.4444444444444444, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.9444444444444444, 0.6666666666666666, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 1.0, 0.6111111111111112, 0.8888888888888888, 1.0, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 1.0, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.4444444444444444, 0.8333333333333334, 0.7777777777777778, 0.6111111111111112, 0.9444444444444444, 0.6111111111111112, 0.6666666666666666, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 1.0, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.5, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.4444444444444444, 0.6111111111111112, 0.9444444444444444, 0.8333333333333334, 1.0, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 1.0, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.6666666666666666, 0.5, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.6111111111111112, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.5, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 1.0, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 1.0, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.6666666666666666, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 1.0, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.6666666666666666, 1.0, 0.9444444444444444, 0.8333333333333334, 0.5555555555555556, 0.7777777777777778, 0.8888888888888888, 1.0, 0.7222222222222222, 1.0, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.6111111111111112, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.5, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.5555555555555556, 0.5555555555555556, 0.9444444444444444, 1.0, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 1.0, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 1.0, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 1.0, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.6111111111111112, 0.9444444444444444, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.5, 0.6666666666666666, 0.8333333333333334, 0.5, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.6666666666666666, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 1.0, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8888888888888888, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.9444444444444444, 1.0, 0.9444444444444444, 1.0, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.4444444444444444, 0.7222222222222222, 1.0, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.5555555555555556, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 1.0, 0.8888888888888888, 0.7777777777777778, 1.0, 0.8888888888888888, 0.6111111111111112, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 1.0, 0.8888888888888888, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.3888888888888889, 0.8888888888888888, 0.6666666666666666, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.6111111111111112, 0.9444444444444444, 0.6666666666666666, 0.5, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 1.0, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 1.0, 0.8333333333333334, 1.0, 0.7777777777777778, 0.4444444444444444, 0.7222222222222222, 0.5555555555555556, 1.0, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.6666666666666666, 0.9444444444444444, 0.6666666666666666, 0.8333333333333334]\n"
     ]
    }
   ],
   "source": [
    "# for every document, calculate the matrix, then f1 score\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "for i in range(len(predictions)):\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    for j in range(len(predictions[i])):\n",
    "        if predictions[i][j] == 1:\n",
    "            if j in testing_targets[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if j in testing_targets[i]:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    precision = 0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    recall = 0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0 if precision+recall == 0 else 2*(precision*recall)/(precision+recall)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append((tp+tn)/len(predictions[i]))\n",
    "\n",
    "print(np.mean(np.array(f1_scores)))\n",
    "print(f1_scores)\n",
    "print(np.mean(np.array(accuracies)))\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
